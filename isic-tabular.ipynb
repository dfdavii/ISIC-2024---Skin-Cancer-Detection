{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":63056,"databundleVersionId":9094797,"sourceType":"competition"},{"sourceId":9334206,"sourceType":"datasetVersion","datasetId":5656074},{"sourceId":109932,"sourceType":"modelInstanceVersion","modelInstanceId":92077,"modelId":116280}],"dockerImageVersionId":30747,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-09T14:01:59.987423Z","iopub.execute_input":"2024-09-09T14:01:59.987795Z","iopub.status.idle":"2024-09-09T14:02:01.015731Z","shell.execute_reply.started":"2024-09-09T14:01:59.987763Z","shell.execute_reply":"2024-09-09T14:02:01.014391Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# test = pd.read_csv('/kaggle/input/isic-2024-challenge/test-metadata.csv')\n# train = pd.read_csv('/kaggle/input/isic-2024-challenge/train-metadata.csv')\npath = '/kaggle/input/isic-2024-challenge/'","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:02:39.728003Z","iopub.execute_input":"2024-09-09T14:02:39.728569Z","iopub.status.idle":"2024-09-09T14:02:39.734471Z","shell.execute_reply.started":"2024-09-09T14:02:39.728523Z","shell.execute_reply":"2024-09-09T14:02:39.733472Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport keras\nimport json\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import models\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom tensorflow.keras import backend as K\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom torchvision import transforms\nimport copy\nimport logging as log\nimport random\nimport h5py\nimport os\nimport io\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:02:40.915529Z","iopub.execute_input":"2024-09-09T14:02:40.916009Z","iopub.status.idle":"2024-09-09T14:02:58.113420Z","shell.execute_reply.started":"2024-09-09T14:02:40.915980Z","shell.execute_reply":"2024-09-09T14:02:58.112446Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-09-09 14:02:42.716114: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-09-09 14:02:42.716222: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-09-09 14:02:42.857789: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"def align_columns(train_df, test_df):\n    # Get the columns that are in test_df but not in train_df\n    extra_columns = set(test_df.columns) - set(train_df.columns)\n    \n    # Drop the extra columns from test_df\n    test_df.drop(columns=extra_columns, inplace=True)\n    \n    # Ensure both DataFrames have the same columns in the same order, excluding 'target'\n    common_columns = [col for col in train_df.columns if col != 'target']\n    test_df = test_df[common_columns]\n    \n    return train_df, test_df\n\n# Example usage\n# train_df, test_df = align_columns(train_df, test_df)\n\nimport numpy as np\n\ndef create_features(train_df, test_df):\n    # Define a small constant to avoid division by zero\n    epsilon = 1e-6\n\n    # Check and create 'lesion_size_ratio' if columns exist\n    if 'clin_size_long_diam_mm' in train_df.columns and 'tbp_lv_minorAxisMM' in train_df.columns:\n        train_df['lesion_size_ratio'] = train_df['clin_size_long_diam_mm'] / (train_df['tbp_lv_minorAxisMM'] + epsilon)\n    if 'clin_size_long_diam_mm' in test_df.columns and 'tbp_lv_minorAxisMM' in test_df.columns:\n        test_df['lesion_size_ratio'] = test_df['clin_size_long_diam_mm'] / (test_df['tbp_lv_minorAxisMM'] + epsilon)\n\n    # Check and create 'perimeter_area_ratio' if columns exist\n    if 'tbp_lv_perimeterMM' in train_df.columns and 'tbp_lv_areaMM2' in train_df.columns:\n        train_df['perimeter_area_ratio'] = train_df['tbp_lv_perimeterMM'] / (train_df['tbp_lv_areaMM2'] + epsilon)\n    if 'tbp_lv_perimeterMM' in test_df.columns and 'tbp_lv_areaMM2' in test_df.columns:\n        test_df['perimeter_area_ratio'] = test_df['tbp_lv_perimeterMM'] / (test_df['tbp_lv_areaMM2'] + epsilon)\n\n    # Check and create 'color_contrast_AB' if columns exist\n    if 'tbp_lv_deltaA' in train_df.columns and 'tbp_lv_deltaB' in train_df.columns:\n        train_df['color_contrast_AB'] = np.sqrt(train_df['tbp_lv_deltaA']**2 + train_df['tbp_lv_deltaB']**2)\n    if 'tbp_lv_deltaA' in test_df.columns and 'tbp_lv_deltaB' in test_df.columns:\n        test_df['color_contrast_AB'] = np.sqrt(test_df['tbp_lv_deltaA']**2 + test_df['tbp_lv_deltaB']**2)\n\n    # Check and create 'color_contrast_LB' if columns exist\n    if 'tbp_lv_deltaL' in train_df.columns and 'tbp_lv_deltaLB' in train_df.columns:\n        train_df['color_contrast_LB'] = np.sqrt(train_df['tbp_lv_deltaL']**2 + train_df['tbp_lv_deltaLB']**2)\n    if 'tbp_lv_deltaL' in test_df.columns and 'tbp_lv_deltaLB' in test_df.columns:\n        test_df['color_contrast_LB'] = np.sqrt(test_df['tbp_lv_deltaL']**2 + test_df['tbp_lv_deltaLB']**2)\n\n    # Check and create 'symmetry_asymmetry_ratio' if columns exist\n    if 'tbp_lv_symm_2axis' in train_df.columns and 'tbp_lv_symm_2axis_angle' in train_df.columns:\n        train_df['symmetry_asymmetry_ratio'] = train_df['tbp_lv_symm_2axis'] / (train_df['tbp_lv_symm_2axis_angle'] + epsilon)\n    if 'tbp_lv_symm_2axis' in test_df.columns and 'tbp_lv_symm_2axis_angle' in test_df.columns:\n        test_df['symmetry_asymmetry_ratio'] = test_df['tbp_lv_symm_2axis'] / (test_df['tbp_lv_symm_2axis_angle'] + epsilon)\n\n    columns_to_drop = [\n        'tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLB',\n        'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle', 'clin_size_long_diam_mm',\n        'tbp_lv_minorAxisMM', 'tbp_lv_perimeterMM', 'tbp_lv_areaMM2'\n    ]\n\n    train_df.drop(columns=[col for col in columns_to_drop if col in train_df.columns], inplace=True)\n    test_df.drop(columns=[col for col in columns_to_drop if col in test_df.columns], inplace=True)\n\n    return train_df, test_df\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:02:58.115465Z","iopub.execute_input":"2024-09-09T14:02:58.116135Z","iopub.status.idle":"2024-09-09T14:02:58.131458Z","shell.execute_reply.started":"2024-09-09T14:02:58.116102Z","shell.execute_reply":"2024-09-09T14:02:58.130529Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\ndef load_data(train_path, test_path):\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    return train_df, test_df\n\ndef select_columns(train_df, test_df, trn_selected, test_selected):\n    train_df = train_df[trn_selected]\n    test_df = test_df[test_selected]\n    return train_df, test_df\n\ndef fill_missing_values(df):\n    for column in df.columns:\n        if df[column].isnull().any():\n            df[column] = df[column].fillna(df[column].mode()[0])\n    return df\n\ndef create_features(train_df, test_df):\n    epsilon = 1e-6\n\n    try:\n        if 'clin_size_long_diam_mm' in train_df.columns and 'tbp_lv_minorAxisMM' in train_df.columns:\n            train_df['lesion_size_ratio'] = train_df['clin_size_long_diam_mm'] / (train_df['tbp_lv_minorAxisMM'] + epsilon)\n        if 'clin_size_long_diam_mm' in test_df.columns and 'tbp_lv_minorAxisMM' in test_df.columns:\n            test_df['lesion_size_ratio'] = test_df['clin_size_long_diam_mm'] / (test_df['tbp_lv_minorAxisMM'] + epsilon)\n\n        if 'tbp_lv_perimeterMM' in train_df.columns and 'tbp_lv_areaMM2' in train_df.columns:\n            train_df['perimeter_area_ratio'] = train_df['tbp_lv_perimeterMM'] / (train_df['tbp_lv_areaMM2'] + epsilon)\n        if 'tbp_lv_perimeterMM' in test_df.columns and 'tbp_lv_areaMM2' in test_df.columns:\n            test_df['perimeter_area_ratio'] = test_df['tbp_lv_perimeterMM'] / (test_df['tbp_lv_areaMM2'] + epsilon)\n\n        if 'tbp_lv_deltaA' in train_df.columns and 'tbp_lv_deltaB' in train_df.columns:\n            train_df['color_contrast_AB'] = np.sqrt(train_df['tbp_lv_deltaA']**2 + train_df['tbp_lv_deltaB']**2)\n        if 'tbp_lv_deltaA' in test_df.columns and 'tbp_lv_deltaB' in test_df.columns:\n            test_df['color_contrast_AB'] = np.sqrt(test_df['tbp_lv_deltaA']**2 + test_df['tbp_lv_deltaB']**2)\n\n        if 'tbp_lv_deltaL' in train_df.columns and 'tbp_lv_deltaLB' in train_df.columns:\n            train_df['color_contrast_LB'] = np.sqrt(train_df['tbp_lv_deltaL']**2 + train_df['tbp_lv_deltaLB']**2)\n        if 'tbp_lv_deltaL' in test_df.columns and 'tbp_lv_deltaLB' in test_df.columns:\n            test_df['color_contrast_LB'] = np.sqrt(test_df['tbp_lv_deltaL']**2 + test_df['tbp_lv_deltaLB']**2)\n\n        if 'tbp_lv_symm_2axis' in train_df.columns and 'tbp_lv_symm_2axis_angle' in train_df.columns:\n            train_df['symmetry_asymmetry_ratio'] = train_df['tbp_lv_symm_2axis'] / (train_df['tbp_lv_symm_2axis_angle'] + epsilon)\n        if 'tbp_lv_symm_2axis' in test_df.columns and 'tbp_lv_symm_2axis_angle' in test_df.columns:\n            test_df['symmetry_asymmetry_ratio'] = test_df['tbp_lv_symm_2axis'] / (test_df['tbp_lv_symm_2axis_angle'] + epsilon)\n\n        columns_to_drop = [\n            'tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLB',\n            'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle', 'clin_size_long_diam_mm',\n            'tbp_lv_minorAxisMM', 'tbp_lv_perimeterMM', 'tbp_lv_areaMM2'\n        ]\n\n        train_df.drop(columns=[col for col in columns_to_drop if col in train_df.columns], inplace=True)\n        test_df.drop(columns=[col for col in columns_to_drop if col in test_df.columns], inplace=True)\n\n    except Exception as e:\n        print(f\"Error in create_features: {e}\")\n\n    return train_df, test_df\n\n\n\n\ndef encode_and_scale(train_df, test_df, categorical_cols, numerical_cols):\n    try:\n#         encoder = OneHotEncoder(drop='first', sparse=False)\n        encoder = OneHotEncoder(drop='first', sparse_output=False)\n        encoded_train = pd.DataFrame(encoder.fit_transform(train_df[categorical_cols]), \n                                     columns=encoder.get_feature_names_out(categorical_cols))\n        encoded_test = pd.DataFrame(encoder.transform(test_df[categorical_cols]), \n                                    columns=encoder.get_feature_names_out(categorical_cols))\n\n        train_df = pd.concat([train_df.drop(columns=categorical_cols), encoded_train], axis=1)\n        test_df = pd.concat([test_df.drop(columns=categorical_cols), encoded_test], axis=1)\n\n        scaler = StandardScaler()\n        train_df[numerical_cols] = scaler.fit_transform(train_df[numerical_cols])\n        test_df[numerical_cols] = scaler.transform(test_df[numerical_cols])\n\n        train_df = train_df.astype({col: 'float32' for col in train_df.select_dtypes(include='float64').columns})\n        test_df = test_df.astype({col: 'float32' for col in test_df.select_dtypes(include='float64').columns})\n\n    except Exception as e:\n        print(f\"Error in encode_and_scale: {e}\")\n\n    return train_df, test_df\n\ndef main(train_path, test_path):\n    trn_selected = ['isic_id', 'target', 'patient_id', 'age_approx', 'sex', 'anatom_site_general',\n                    'clin_size_long_diam_mm', 'image_type', 'tbp_tile_type', 'tbp_lv_A', 'tbp_lv_Aext', \n                    'tbp_lv_B', 'tbp_lv_Bext', 'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', \n                    'tbp_lv_L', 'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', \n                    'tbp_lv_color_std_mean', 'tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL', \n                    'tbp_lv_deltaLB', 'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', 'tbp_lv_location', \n                    'tbp_lv_location_simple', 'tbp_lv_minorAxisMM', 'tbp_lv_nevi_confidence', \n                    'tbp_lv_norm_border', 'tbp_lv_norm_color', 'tbp_lv_perimeterMM', \n                    'tbp_lv_radial_color_std_max', 'tbp_lv_stdL', 'tbp_lv_stdLExt', 'tbp_lv_symm_2axis', \n                    'tbp_lv_symm_2axis_angle', 'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z', 'attribution', \n                    'copyright_license']\n\n    test_selected = ['isic_id', 'patient_id', 'age_approx', 'sex', 'anatom_site_general',\n                     'clin_size_long_diam_mm', 'image_type', 'tbp_tile_type', 'tbp_lv_A', 'tbp_lv_Aext', \n                     'tbp_lv_B', 'tbp_lv_Bext', 'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', \n                     'tbp_lv_L', 'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', \n                     'tbp_lv_color_std_mean', 'tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL', \n                     'tbp_lv_deltaLB', 'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', 'tbp_lv_location', \n                     'tbp_lv_location_simple', 'tbp_lv_minorAxisMM', 'tbp_lv_nevi_confidence', \n                     'tbp_lv_norm_border', 'tbp_lv_norm_color', 'tbp_lv_perimeterMM', \n                     'tbp_lv_radial_color_std_max', 'tbp_lv_stdL', 'tbp_lv_stdLExt', 'tbp_lv_symm_2axis', \n                     'tbp_lv_symm_2axis_angle', 'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z', 'attribution', \n                     'copyright_license']\n\n    categorical_cols = ['sex', 'anatom_site_general', 'image_type', 'tbp_tile_type', \n                        'tbp_lv_location', 'tbp_lv_location_simple']\n\n    numerical_cols = ['age_approx', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext',\n                      'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L',\n                      'tbp_lv_Lext', 'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean',\n                      'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', 'tbp_lv_nevi_confidence',\n                      'tbp_lv_norm_border', 'tbp_lv_norm_color', 'tbp_lv_radial_color_std_max',\n                      'tbp_lv_stdL', 'tbp_lv_stdLExt', 'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z',\n                      'lesion_size_ratio', 'perimeter_area_ratio', 'color_contrast_AB',\n                      'color_contrast_LB', 'symmetry_asymmetry_ratio']\n\n    train_df, test_df = load_data(train_path, test_path)\n    train_df, test_df = select_columns(train_df, test_df, trn_selected, test_selected)\n    train_df = fill_missing_values(train_df)\n    test_df = fill_missing_values(test_df)\n    train_df, test_df = create_features(train_df, test_df)\n    train_df, test_df = encode_and_scale(train_df, test_df, categorical_cols, numerical_cols)\n\n    train_df.drop(['attribution', 'copyright_license'], axis=1, inplace=True)\n    test_df.drop(['attribution', 'copyright_license'], axis=1, inplace=True)\n\n    return train_df, test_df\n\n# Example usage\ntrain_path = '/kaggle/input/isic-2024-challenge/train-metadata.csv'\ntest_path = '/kaggle/input/isic-2024-challenge/test-metadata.csv'\ntrain_df, test_df = main(train_path, test_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:02:58.132961Z","iopub.execute_input":"2024-09-09T14:02:58.133259Z","iopub.status.idle":"2024-09-09T14:03:08.043353Z","shell.execute_reply.started":"2024-09-09T14:02:58.133234Z","shell.execute_reply":"2024-09-09T14:03:08.042545Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# train_df","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:03:08.046061Z","iopub.execute_input":"2024-09-09T14:03:08.046651Z","iopub.status.idle":"2024-09-09T14:03:08.050667Z","shell.execute_reply.started":"2024-09-09T14:03:08.046607Z","shell.execute_reply":"2024-09-09T14:03:08.049660Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from collections import namedtuple\n\nCandidateInfoTuple = namedtuple(\n    'CandidateInfoTuple',\n    'isNodule_bool, isic_id, patient_id, attributes',\n)\n\n\ndef getCandidateInfoList(df, is_train=True, requireOnDisk_bool=True):\n    candidateInfo_list = []\n\n    # Define columns to drop based on whether it's train or test DataFrame\n    if is_train:\n        columns_to_drop = ['isic_id', 'target', 'patient_id']\n    else:\n        columns_to_drop = ['isic_id', 'patient_id']\n\n    # Iterate over each row in the dataframe\n    for _, row in df.iterrows():\n        # Drop unnecessary columns to get attributes\n        attributes = row.drop(columns_to_drop).tolist()\n\n        # Wrap each attribute in a single-element list\n#         attributes = [[attr] for attr in attributes]\n\n        # Determine the value of isNodule_bool, if 'target' exists\n        isNodule_bool = row['target'] if 'target' in df.columns else None\n\n        # Append the CandidateInfoTuple to the list\n        candidateInfo_list.append(CandidateInfoTuple(\n            isNodule_bool=isNodule_bool,\n            isic_id=row['isic_id'],\n            patient_id=row['patient_id'],\n            attributes=attributes,\n        ))\n\n    # Sort the candidate info list in reverse order\n    candidateInfo_list.sort(reverse=True)\n    return candidateInfo_list\n\n\n\n# Usage example:\n# metadata_df = pd.read_csv('train_metadata.csv')\n# candidates = getCandidateInfoList()\n# print(candidates[0])  # Modify as needed","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:03:08.051855Z","iopub.execute_input":"2024-09-09T14:03:08.052443Z","iopub.status.idle":"2024-09-09T14:03:08.062043Z","shell.execute_reply.started":"2024-09-09T14:03:08.052412Z","shell.execute_reply":"2024-09-09T14:03:08.061253Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Img:\n    def __init__(self, isic_id, img_path, transform=None):\n        with h5py.File(img_path, 'r') as f:\n            try:\n                dset = f[isic_id]\n                self.image_data = dset[()]\n                image_data_io = io.BytesIO(self.image_data)\n                self.image = Image.open(image_data_io)\n\n                # Apply transform if provided\n                if transform:\n                    self.image = transform(self.image)\n                    self.image_array = self.image.numpy()  # Convert the transformed tensor to a NumPy array\n\n                    # The transform will already convert the image to (C, H, W) when using ToTensor\n                    # No need to transpose here if ToTensor was applied\n                else:\n                    # No transform; handle the image normally\n                    self.image_array = np.array(self.image)\n                    if self.image_array.dtype == np.uint8:\n                        self.image_array = self.image_array / 255.0  # Normalize for display\n\n                    # Transpose from (H, W, C) to (C, H, W) if necessary\n                    if self.image_array.shape[-1] == 3:  # check if last dimension is channels\n                        self.image_array = np.transpose(self.image_array, (2, 0, 1))\n\n#                 print(f\"Final shape: {self.image_array.shape}\")\n\n            except KeyError:\n                raise ValueError(f\"ISIC ID '{isic_id}' not found in 'train-image.hdf5'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:03:08.063090Z","iopub.execute_input":"2024-09-09T14:03:08.063441Z","iopub.status.idle":"2024-09-09T14:03:08.075679Z","shell.execute_reply.started":"2024-09-09T14:03:08.063416Z","shell.execute_reply":"2024-09-09T14:03:08.074673Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# /kaggle/input/isic-skin-dset\n# def load_npy(isic_id, img_path):\n#     return np.load(os.path.join(img_path, f\"{isic_id}.npy\"), mmap_mode='r')\n\n\ndef load_npy(isic_id, img_path):\n    \"\"\"\n    Load the .npy file corresponding to the given ISIC ID.\n\n    Args:\n        isic_id (str): The ISIC ID of the image.\n        img_path (str): The directory path where the .npy files are stored.\n\n    Returns:\n        np.ndarray: The loaded image data.\n    \"\"\"\n    file_path = os.path.join(img_path, f\"{isic_id}.npy\")\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"File not found: {file_path}\")\n    return np.load(file_path)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:03:08.076926Z","iopub.execute_input":"2024-09-09T14:03:08.077261Z","iopub.status.idle":"2024-09-09T14:03:08.087255Z","shell.execute_reply.started":"2024-09-09T14:03:08.077227Z","shell.execute_reply":"2024-09-09T14:03:08.086469Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nfrom PIL import Image\n\nclass ImgDatasetTF:\n    def __init__(self,\n                 val_stride=0,\n                 isValSet_bool=None,\n                 isic_id=None,\n                 img_path=None,\n                 sortby_str='random',\n                 ratio_int=0,\n                 is_train=None,\n                 train_df=None,\n                 transform=None,\n                 chunksize=10000):\n        self.is_train = is_train\n        self.train_df = train_df\n        self.img_path = img_path\n        self.transform = transform\n        self.chunksize = chunksize\n        self.ratio_int = ratio_int\n        self.imgInfo_list = []\n\n        # Process the DataFrame in chunks\n        for start in range(0, len(self.train_df), self.chunksize):\n            chunk = train_df.iloc[start:start + self.chunksize]\n            self.imgInfo_list.extend(getCandidateInfoList(chunk, self.is_train))\n\n        # Filter by isic_id if provided\n        if isic_id:\n            self.imgInfo_list = [x for x in self.imgInfo_list if x.isic_id == isic_id]\n\n        # Validation set handling\n        if isValSet_bool:\n            assert val_stride > 0, val_stride\n            self.imgInfo_list = self.imgInfo_list[::val_stride]\n            assert self.imgInfo_list\n        elif val_stride > 0:\n            del self.imgInfo_list[::val_stride]\n            assert self.imgInfo_list\n\n        # Sort by the specified method\n        if sortby_str == 'random':\n            random.shuffle(self.imgInfo_list)\n\n        # Split into positive and negative lists if ratio is specified\n        self.negative_list = [nt for nt in self.imgInfo_list if not nt.isNodule_bool]\n        self.pos_list = [nt for nt in self.imgInfo_list if nt.isNodule_bool]\n        \n        # Automatically determine number of attributes from the first sample\n        if self.imgInfo_list:\n            self.num_attributes = len(self.imgInfo_list[0].attributes)\n        else:\n            self.num_attributes = 62 \n        \n    def shuffleSamples(self):\n        if self.ratio_int:\n            random.shuffle(self.negative_list)\n            random.shuffle(self.pos_list)\n\n    def __len__(self):\n        if self.ratio_int:\n            return 200000  # Or some other large number if balancing ratio\n        return len(self.imgInfo_list)\n    \n    def to_tf_dataset(self, batch_size=32):\n        def generator():\n            for idx in range(len(self)):\n                attributes, isic_id, patient_id, labels = self[idx]\n                yield attributes, isic_id, patient_id, labels\n\n        # Update the output_signature to include all 4 values\n        dataset = tf.data.Dataset.from_generator(generator, \n                                                 output_signature=(\n                                                     tf.TensorSpec(shape=(self.num_attributes,), dtype=tf.float32),  # attributes\n                                                     tf.TensorSpec(shape=(), dtype=tf.string),  # isic_id\n                                                     tf.TensorSpec(shape=(), dtype=tf.string),  # patient_id\n                                                     tf.TensorSpec(shape=(2,), dtype=tf.float32)  # labels\n                                                 ))\n        dataset = dataset.batch(batch_size)\n        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n        return dataset\n\n    def __getitem__(self, ndx):\n        # Handle ratio balancing between positive and negative samples\n        if self.ratio_int:\n            pos_ndx = ndx // (self.ratio_int + 1)\n            if ndx % (self.ratio_int + 1):\n                neg_ndx = ndx - 1 - pos_ndx\n                neg_ndx %= len(self.negative_list)\n                imgInfo_tup = self.negative_list[neg_ndx]\n            else:\n                pos_ndx %= len(self.pos_list)\n                imgInfo_tup = self.pos_list[pos_ndx]\n        else:\n            imgInfo_tup = self.imgInfo_list[ndx]\n\n        # Load the image using the ISIC ID on-the-fly\n#         img_t = load_npy(imgInfo_tup.isic_id, self.img_path)\n\n#         # Convert the NumPy array to a PIL Image\n#         img_t = Image.fromarray(img_t)\n\n#         # Apply transformations if provided\n#         if self.transform:\n#             img_t = self.transform(img_t)\n\n#         # Convert image to TensorFlow tensor\n#         img_t = tf.convert_to_tensor(np.array(img_t), dtype=tf.float32)\n\n        # Convert attributes to a NumPy array\n        # Convert attributes to a NumPy array and reshape to (62,)\n        attributes = np.array(imgInfo_tup.attributes, dtype=np.float32).reshape(-1)\n#         label = imgInfo_tup.label if hasattr(imgInfo_tup, 'label') else None\n        labels = np.array([1, 0], dtype=np.float32) if imgInfo_tup.isNodule_bool else np.array([0, 1], dtype=np.float32)\n    \n        return attributes, imgInfo_tup.isic_id, imgInfo_tup.patient_id, labels\n\n#         return img_t, imgInfo_tup.isic_id, imgInfo_tup.patient_id, attributes, labels\n\n\n\n# Define transformations for training (no resize, just random flip)\ndef transform_train(img):\n    img = tf.image.random_flip_left_right(img)  # Randomly flip horizontally\n    img = tf.cast(img, tf.float32)  # Cast to float32\n    img = img / 255.0  # Normalize to [0, 1]\n    return img\n\n\n# Define transformations for testing (include resize)\ndef transform_test(img):\n    img = tf.image.resize(img, [48, 48])  # Resize to 48x48\n    img = tf.cast(img, tf.float32)  # Cast to float32\n#     img = img / 255.0  # Normalize to [0, 1]\n    return img\n\n\n# Load an example image from the dataset and display it\ndef display_image_from_dataset(img_dataset, index):\n    img_t, isic_id, patient_id, attributes = img_dataset[index]\n    \n    # Convert the TensorFlow tensor back to NumPy for visualization\n    img_np = img_t.numpy()\n\n    # Plot the image using matplotlib\n    plt.imshow(img_np)\n    plt.title(f\"ISIC ID: {isic_id}, Patient ID: {patient_id}\")\n    plt.axis('off')\n    plt.show()\n\n\n# Example usage:\n# Initialize the dataset (use test_df and img_path from your environment)\n# img_dataset_train = ImgDatasetTF(transform=transform_train, train_df=train_df, img_path=\"/kaggle/input/training/npy_images\")\n\n# Display one image at index 0\n# display_image_from_dataset(img_dataset_train, index=0)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:03:08.088225Z","iopub.execute_input":"2024-09-09T14:03:08.090358Z","iopub.status.idle":"2024-09-09T14:03:08.113819Z","shell.execute_reply.started":"2024-09-09T14:03:08.090327Z","shell.execute_reply":"2024-09-09T14:03:08.112947Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\nimport numpy as np\nfrom tqdm import tqdm\n\n# Function to calculate metrics\ndef calculate_metrics(true_labels, predicted, num_correct_neg, total_neg, num_correct_pos, total_pos):\n    if len(true_labels.shape) > 1:\n        true_labels = np.argmax(true_labels, axis=1)\n    if len(predicted.shape) > 1:\n        predicted = np.argmax(predicted, axis=1)\n\n    cm = confusion_matrix(true_labels, predicted)\n    \n    if cm.shape == (2, 2):\n        tn, fp, fn, tp = cm.ravel()\n    else:\n        tn = fp = fn = tp = 0\n\n    precision = precision_score(true_labels, predicted, zero_division=0)\n    recall = recall_score(true_labels, predicted, zero_division=0)\n    f1 = f1_score(true_labels, predicted, zero_division=0)\n\n    acc = 100.0 * (num_correct_neg + num_correct_pos) / (total_neg + total_pos)\n\n    print(f'TN={tn}, FP={fp}, FN={fn}, TP={tp}, Precision={precision:.2f}, Recall={recall:.2f}, F1={f1:.2f}')\n    print(f'Accuracy: {acc:.2f}% ({num_correct_neg} of {total_neg} negative, {num_correct_pos} of {total_pos} positive)')\n\n# Training and validation function with early stopping\ndef train_model(model, train_dataset, val_dataset, epochs, optimizer, loss_fn, patience=3):\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(epochs):\n        # Training phase\n        running_loss = 0.0\n        num_correct_neg = num_correct_pos = total_neg = total_pos = 0\n        num_samples = 0\n\n        for attributes, isic_id, patient_id, labels in tqdm(train_dataset):\n            with tf.GradientTape() as tape:\n                outputs = model(attributes, training=True)\n                loss = loss_fn(labels, outputs)\n                running_loss += loss.numpy()\n\n            grads = tape.gradient(loss, model.trainable_variables)\n            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n            predicted = tf.argmax(outputs, axis=1)\n            true_labels = tf.argmax(labels, axis=1)\n\n            correct_neg = tf.reduce_sum(tf.cast((predicted == 0) & (true_labels == 0), tf.int32)).numpy()\n            correct_pos = tf.reduce_sum(tf.cast((predicted == 1) & (true_labels == 1), tf.int32)).numpy()\n            num_correct_neg += correct_neg\n            num_correct_pos += correct_pos\n            total_neg += tf.reduce_sum(tf.cast(true_labels == 0, tf.int32)).numpy()\n            total_pos += tf.reduce_sum(tf.cast(true_labels == 1, tf.int32)).numpy()\n            num_samples += 1\n\n        print(f'Epoch {epoch+1}/{epochs} - Training Loss: {running_loss / num_samples}')\n        calculate_metrics(true_labels.numpy(), predicted.numpy(), num_correct_neg, total_neg, num_correct_pos, total_pos)\n\n        # Validation phase\n        running_loss_val = 0.0\n        num_correct_neg_val = num_correct_pos_val = total_neg_val = total_pos_val = 0\n        num_samples_val = 0\n\n        for attributes, isic_id, patient_id, labels in tqdm(val_dataset):\n            outputs = model(attributes, training=False)\n            loss_val = loss_fn(labels, outputs)\n            running_loss_val += loss_val.numpy()\n\n            predicted_val = tf.argmax(outputs, axis=1)\n            true_labels_val = tf.argmax(labels, axis=1)\n\n            correct_neg_val = tf.reduce_sum(tf.cast((predicted_val == 0) & (true_labels_val == 0), tf.int32)).numpy()\n            correct_pos_val = tf.reduce_sum(tf.cast((predicted_val == 1) & (true_labels_val == 1), tf.int32)).numpy()\n            num_correct_neg_val += correct_neg_val\n            num_correct_pos_val += correct_pos_val\n            total_neg_val += tf.reduce_sum(tf.cast(true_labels_val == 0, tf.int32)).numpy()\n            total_pos_val += tf.reduce_sum(tf.cast(true_labels_val == 1, tf.int32)).numpy()\n            num_samples_val += 1\n\n        val_loss = running_loss_val / num_samples_val\n        print(f'Epoch {epoch+1}/{epochs} - Validation Loss: {val_loss}')\n        calculate_metrics(true_labels_val.numpy(), predicted_val.numpy(), num_correct_neg_val, total_neg_val, num_correct_pos_val, total_pos_val)\n\n        # Early stopping logic\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"Early stopping triggered\")\n                break\n\n# Example usage:\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:03:08.115227Z","iopub.execute_input":"2024-09-09T14:03:08.115544Z","iopub.status.idle":"2024-09-09T14:03:08.211715Z","shell.execute_reply.started":"2024-09-09T14:03:08.115509Z","shell.execute_reply":"2024-09-09T14:03:08.210930Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers, losses, metrics\n\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n\nclass MyModel(tf.keras.Model):\n    def __init__(self, num_attributes):\n        super(MyModel, self).__init__()\n        self.num_attributes = num_attributes  # Save this parameter for re-creation\n\n        # Model for attributes\n        self.attributes_model = tf.keras.Sequential([\n            layers.Dense(64, activation='relu'),\n            layers.Dense(32, activation='relu'),\n            layers.Dense(2, activation='softmax')  # Output layer for binary classification\n        ])\n\n    def call(self, attributes):\n        # Attributes model forward pass\n        outputs = self.attributes_model(attributes)\n        return outputs\n\n    def get_config(self):\n        # Return a dictionary of parameters to reconstruct the model\n        config = super(MyModel, self).get_config()\n        config.update({\n            'num_attributes': self.num_attributes\n        })\n        return config\n\n    @classmethod\n    def from_config(cls, config):\n        # Recreate the model from the configuration\n        return cls(num_attributes=config['num_attributes'])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:03:08.214318Z","iopub.execute_input":"2024-09-09T14:03:08.214655Z","iopub.status.idle":"2024-09-09T14:03:08.223352Z","shell.execute_reply.started":"2024-09-09T14:03:08.214627Z","shell.execute_reply":"2024-09-09T14:03:08.222402Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model = MyModel(num_attributes= 62)  # Define your model here\noptimizer = tf.optimizers.Adam()\nloss_fn = tf.losses.CategoricalCrossentropy()\n\nmg_path = \"/kaggle/input/training/npy_images\"\n\ntrain_dataset = ImgDatasetTF(val_stride=10, isValSet_bool=False,\n                             img_path=None, ratio_int=1, is_train=True, train_df=train_df)\nval_dataset = ImgDatasetTF(val_stride=10, isValSet_bool=True,\n                           img_path=mg_path, is_train=True, train_df=train_df)\n\n\n# Train and validation dataset created from ImgDatasetTF class\ntrain_tf_dataset = train_dataset.to_tf_dataset(batch_size=32)\nval_tf_dataset = val_dataset.to_tf_dataset(batch_size=32)\n\ntrain_model(model, train_tf_dataset, val_tf_dataset, epochs=5, optimizer=optimizer, loss_fn=loss_fn, patience=3)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:03:08.224685Z","iopub.execute_input":"2024-09-09T14:03:08.225024Z","iopub.status.idle":"2024-09-09T14:33:02.902210Z","shell.execute_reply.started":"2024-09-09T14:03:08.224992Z","shell.execute_reply":"2024-09-09T14:33:02.901265Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"6250it [04:45, 21.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5 - Training Loss: 0.09819168377641588\nTN=16, FP=0, FN=2, TP=14, Precision=1.00, Recall=0.88, F1=0.93\nAccuracy: 97.07% (98456 of 100000 negative, 95693 of 100000 positive)\n","output_type":"stream"},{"name":"stderr","text":"1254it [00:21, 58.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5 - Validation Loss: 0.05663769093195423\nTN=0, FP=0, FN=0, TP=0, Precision=1.00, Recall=1.00, F1=1.00\nAccuracy: 98.27% (32 of 60 negative, 39379 of 40046 positive)\n","output_type":"stream"},{"name":"stderr","text":"6250it [04:42, 22.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5 - Training Loss: 0.032484279160373844\nTN=16, FP=0, FN=2, TP=14, Precision=1.00, Recall=0.88, F1=0.93\nAccuracy: 99.22% (99851 of 100000 negative, 98587 of 100000 positive)\n","output_type":"stream"},{"name":"stderr","text":"1254it [00:21, 58.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5 - Validation Loss: 0.0390185378759621\nTN=0, FP=0, FN=0, TP=0, Precision=1.00, Recall=1.00, F1=1.00\nAccuracy: 99.06% (24 of 60 negative, 39703 of 40046 positive)\n","output_type":"stream"},{"name":"stderr","text":"6250it [04:42, 22.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5 - Training Loss: 0.021690991896744817\nTN=16, FP=0, FN=2, TP=14, Precision=1.00, Recall=0.88, F1=0.93\nAccuracy: 99.48% (99860 of 100000 negative, 99097 of 100000 positive)\n","output_type":"stream"},{"name":"stderr","text":"1254it [00:21, 58.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5 - Validation Loss: 0.04895432837956788\nTN=0, FP=0, FN=0, TP=0, Precision=1.00, Recall=1.00, F1=1.00\nAccuracy: 98.81% (23 of 60 negative, 39605 of 40046 positive)\n","output_type":"stream"},{"name":"stderr","text":"6250it [04:41, 22.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5 - Training Loss: 0.01610160616343608\nTN=16, FP=0, FN=1, TP=15, Precision=1.00, Recall=0.94, F1=0.97\nAccuracy: 99.61% (99873 of 100000 negative, 99338 of 100000 positive)\n","output_type":"stream"},{"name":"stderr","text":"1254it [00:21, 58.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5 - Validation Loss: 0.039431468885815314\nTN=0, FP=0, FN=1, TP=9, Precision=1.00, Recall=0.90, F1=0.95\nAccuracy: 99.06% (19 of 60 negative, 39710 of 40046 positive)\n","output_type":"stream"},{"name":"stderr","text":"6250it [04:43, 22.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5 - Training Loss: 0.01291537980471563\nTN=16, FP=0, FN=1, TP=15, Precision=1.00, Recall=0.94, F1=0.97\nAccuracy: 99.68% (99875 of 100000 negative, 99493 of 100000 positive)\n","output_type":"stream"},{"name":"stderr","text":"1254it [00:21, 57.41it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5 - Validation Loss: 0.04218070611868397\nTN=0, FP=0, FN=1, TP=9, Precision=1.00, Recall=0.90, F1=0.95\nAccuracy: 99.12% (14 of 60 negative, 39740 of 40046 positive)\nEarly stopping triggered\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load your entire dataset\n# dataset = ImgDatasetTF(val_stride=10, isValSet_bool=False, img_path=None, ratio_int=1, is_train=True, train_df=train_df)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:30:30.935019Z","iopub.execute_input":"2024-09-09T13:30:30.935667Z","iopub.status.idle":"2024-09-09T13:33:02.069356Z","shell.execute_reply.started":"2024-09-09T13:30:30.935631Z","shell.execute_reply":"2024-09-09T13:33:02.068411Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import KFold\n\n\n# kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n#     print(f\"Fold {fold+1}\")\n\n#     # Split the dataset into training and validation sets\n#     train_data = np.array(dataset)[train_idx]\n#     val_data = np.array(dataset)[val_idx]\n\n#     # Convert to TensorFlow datasets\n#     train_tf_dataset = to_tf_dataset(train_data, batch_size=32)\n#     val_tf_dataset = to_tf_dataset(val_data, batch_size=32)\n\n#     # Apply optimizations\n#     train_tf_dataset = train_tf_dataset.shuffle(buffer_size=10000).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n#     val_tf_dataset = val_tf_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n#     # Train the model\n#     train_model(model, train_tf_dataset, val_tf_dataset, epochs=5, optimizer=optimizer, loss_fn=loss_fn, patience=3)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T13:43:45.512887Z","iopub.execute_input":"2024-09-09T13:43:45.513581Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Fold 1\n","output_type":"stream"}]},{"cell_type":"code","source":"model_json = model.to_json()\nwith open(\"model_skin2.json\", \"w\") as json_file:\n    json_file.write(model_json)\nmodel.save_weights(\"/kaggle/working/model_skin2.weights.h5\")\n\n# Load the model architecture and weights separately\nfrom tensorflow.keras.models import model_from_json\n\nwith open(\"/kaggle/working/model_skin2.json\", \"r\") as json_file:\n    loaded_model_json = json_file.read()\nloaded_model = model_from_json(loaded_model_json, custom_objects={'MyModel': MyModel})\nloaded_model.load_weights(\"/kaggle/working/model_skin2.weights.h5\")","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:34:26.801176Z","iopub.execute_input":"2024-09-09T14:34:26.801570Z","iopub.status.idle":"2024-09-09T14:34:26.889430Z","shell.execute_reply.started":"2024-09-09T14:34:26.801540Z","shell.execute_reply":"2024-09-09T14:34:26.888644Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# # Specify the directory where the model will be saved\n# model_save_path = '/kaggle/working/model_skin3.h5'\n\n# # Save the model\n# model.save(model_save_path)\n# print(f\"Model saved at {model_save_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T19:50:50.659609Z","iopub.execute_input":"2024-09-08T19:50:50.659916Z","iopub.status.idle":"2024-09-08T19:50:50.663809Z","shell.execute_reply.started":"2024-09-08T19:50:50.659892Z","shell.execute_reply":"2024-09-08T19:50:50.662859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport copy\n\nclass TestDataset:\n    def __init__(self, isic_id=None, img_path=None, is_train=None, test_df=None, transform=None):\n        self.test_df = test_df\n        self.is_train = is_train\n        self.imgInfo_list = copy.copy(getCandidateInfoList(self.test_df, self.is_train))\n        self.img_path = img_path\n        self.transform = transform\n\n        if isic_id:\n            self.imgInfo_list = [x for x in self.imgInfo_list if x.isic_id == isic_id]\n\n    def generator(self):\n        for imgInfo_tup in self.imgInfo_list:\n            attributes = np.array(imgInfo_tup.attributes, dtype=np.float32)\n            yield imgInfo_tup.isic_id, imgInfo_tup.patient_id, attributes\n\n    def to_tf_dataset(self):\n        return tf.data.Dataset.from_generator(\n            self.generator,\n            output_signature=(\n                tf.TensorSpec(shape=(), dtype=tf.string),  # ISIC ID\n                tf.TensorSpec(shape=(), dtype=tf.string),  # Patient ID\n                tf.TensorSpec(shape=(len(self.imgInfo_list[0].attributes),), dtype=tf.float32)  # Attributes\n            )\n        )\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:35:22.481852Z","iopub.execute_input":"2024-09-09T14:35:22.482464Z","iopub.status.idle":"2024-09-09T14:35:22.492668Z","shell.execute_reply.started":"2024-09-09T14:35:22.482433Z","shell.execute_reply":"2024-09-09T14:35:22.491745Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\nimport gc\n\ndef predict(model, dataset, batch_size=32, device='cpu'):\n    results = []\n    \n    device = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'\n\n    # Explicitly specify the device (GPU or CPU)\n    with tf.device(device):\n        for batch in tqdm(dataset.batch(batch_size)):\n            isic_id, patient_id, attributes = batch\n            \n            # Decode byte strings to regular strings\n            isic_id = [id.decode('utf-8') for id in isic_id.numpy()]\n\n            # Move data to the specified device automatically based on model's device\n            outputs = model(attributes, training=False)  # Inference mode\n\n            # Apply softmax to the outputs to get probabilities\n            probs = tf.nn.softmax(outputs, axis=1).numpy()\n\n            # Extract malignant class probabilities (assuming class 1 is malignant)\n            malignant_probs = probs[:, 1]\n\n            # Round the probabilities to one decimal place\n            malignant_probs = np.round(malignant_probs, 1)\n\n            # Store the ISIC IDs and corresponding probabilities\n            results.extend(zip(isic_id, malignant_probs))\n\n            # Perform garbage collection\n            gc.collect()\n\n    return results\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:35:34.445821Z","iopub.execute_input":"2024-09-09T14:35:34.446189Z","iopub.status.idle":"2024-09-09T14:35:34.454773Z","shell.execute_reply.started":"2024-09-09T14:35:34.446161Z","shell.execute_reply":"2024-09-09T14:35:34.453821Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Create an instance of the TestDataset\ntest_dataset = TestDataset(is_train=False, img_path=None, test_df=test_df).to_tf_dataset()\n\n\n# Run the prediction function\npredictions = predict(loaded_model, test_dataset, batch_size=32)\n\n\n# predictions\n","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:37:11.841376Z","iopub.execute_input":"2024-09-09T14:37:11.841757Z","iopub.status.idle":"2024-09-09T14:37:14.017322Z","shell.execute_reply.started":"2024-09-09T14:37:11.841728Z","shell.execute_reply":"2024-09-09T14:37:14.016444Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"1it [00:02,  2.14s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.DataFrame(predictions, columns=['isic_id', 'target'])","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:37:21.010393Z","iopub.execute_input":"2024-09-09T14:37:21.011336Z","iopub.status.idle":"2024-09-09T14:37:21.016540Z","shell.execute_reply.started":"2024-09-09T14:37:21.011300Z","shell.execute_reply":"2024-09-09T14:37:21.015568Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:37:26.379399Z","iopub.execute_input":"2024-09-09T14:37:26.380167Z","iopub.status.idle":"2024-09-09T14:37:26.393289Z","shell.execute_reply.started":"2024-09-09T14:37:26.380136Z","shell.execute_reply":"2024-09-09T14:37:26.392375Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"        isic_id  target\n0  ISIC_0015740     0.7\n1  ISIC_0015729     0.7\n2  ISIC_0015657     0.7","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>isic_id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ISIC_0015740</td>\n      <td>0.7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ISIC_0015729</td>\n      <td>0.7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ISIC_0015657</td>\n      <td>0.7</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-09T14:37:35.265071Z","iopub.execute_input":"2024-09-09T14:37:35.265798Z","iopub.status.idle":"2024-09-09T14:37:35.272212Z","shell.execute_reply.started":"2024-09-09T14:37:35.265767Z","shell.execute_reply":"2024-09-09T14:37:35.271412Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}